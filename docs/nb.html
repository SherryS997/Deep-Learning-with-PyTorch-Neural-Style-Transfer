<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neural Image Style Transfer using PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-and-overview" id="toc-introduction-and-overview" class="nav-link active" data-scroll-target="#introduction-and-overview">Introduction and Overview</a></li>
  <li><a href="#loading-and-preparing-pretrained-vgg19-model-for-neural-style-transfer" id="toc-loading-and-preparing-pretrained-vgg19-model-for-neural-style-transfer" class="nav-link" data-scroll-target="#loading-and-preparing-pretrained-vgg19-model-for-neural-style-transfer">Loading and Preparing Pretrained VGG19 Model for Neural Style Transfer</a>
  <ul class="collapse">
  <li><a href="#code-and-its-description" id="toc-code-and-its-description" class="nav-link" data-scroll-target="#code-and-its-description">Code and its Description:</a></li>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose:</a></li>
  </ul></li>
  <li><a href="#image-preprocessing-transforming-raw-images-into-compatible-tensors" id="toc-image-preprocessing-transforming-raw-images-into-compatible-tensors" class="nav-link" data-scroll-target="#image-preprocessing-transforming-raw-images-into-compatible-tensors">Image Preprocessing: Transforming Raw Images into Compatible Tensors</a>
  <ul class="collapse">
  <li><a href="#code-and-its-description-1" id="toc-code-and-its-description-1" class="nav-link" data-scroll-target="#code-and-its-description-1">Code and its Description:</a></li>
  </ul></li>
  <li><a href="#image-deprocessing-transforming-normalized-tensors-to-displayable-images" id="toc-image-deprocessing-transforming-normalized-tensors-to-displayable-images" class="nav-link" data-scroll-target="#image-deprocessing-transforming-normalized-tensors-to-displayable-images">Image Deprocessing: Transforming Normalized Tensors to Displayable Images</a></li>
  <li><a href="#feature-extraction-and-gram-matrix-calculation-analyzing-image-features-for-style-representation" id="toc-feature-extraction-and-gram-matrix-calculation-analyzing-image-features-for-style-representation" class="nav-link" data-scroll-target="#feature-extraction-and-gram-matrix-calculation-analyzing-image-features-for-style-representation">Feature Extraction and Gram Matrix Calculation: Analyzing Image Features for Style Representation</a></li>
  <li><a href="#defining-loss-functions-for-neural-style-transfer" id="toc-defining-loss-functions-for-neural-style-transfer" class="nav-link" data-scroll-target="#defining-loss-functions-for-neural-style-transfer">Defining Loss Functions for Neural Style Transfer</a>
  <ul class="collapse">
  <li><a href="#content-loss-function" id="toc-content-loss-function" class="nav-link" data-scroll-target="#content-loss-function">Content Loss Function:</a></li>
  <li><a href="#style-loss-function" id="toc-style-loss-function" class="nav-link" data-scroll-target="#style-loss-function">Style Loss Function:</a></li>
  <li><a href="#optimization-and-total-loss-function" id="toc-optimization-and-total-loss-function" class="nav-link" data-scroll-target="#optimization-and-total-loss-function">Optimization and Total Loss Function:</a></li>
  </ul></li>
  <li><a href="#training-loop-iterative-optimization-for-neural-style-transfer" id="toc-training-loop-iterative-optimization-for-neural-style-transfer" class="nav-link" data-scroll-target="#training-loop-iterative-optimization-for-neural-style-transfer">Training Loop: Iterative Optimization for Neural Style Transfer</a>
  <ul class="collapse">
  <li><a href="#code-and-its-description-2" id="toc-code-and-its-description-2" class="nav-link" data-scroll-target="#code-and-its-description-2">Code and its Description:</a></li>
  </ul></li>
  <li><a href="#results-visualization-displaying-stages-of-style-transfer-process" id="toc-results-visualization-displaying-stages-of-style-transfer-process" class="nav-link" data-scroll-target="#results-visualization-displaying-stages-of-style-transfer-process">Results Visualization: Displaying Stages of Style Transfer Process</a>
  <ul class="collapse">
  <li><a href="#code-and-its-description-3" id="toc-code-and-its-description-3" class="nav-link" data-scroll-target="#code-and-its-description-3">Code and its Description:</a></li>
  </ul></li>
  <li><a href="#final-results-comparison-of-stylized-image-and-original-content" id="toc-final-results-comparison-of-stylized-image-and-original-content" class="nav-link" data-scroll-target="#final-results-comparison-of-stylized-image-and-original-content">Final Results: Comparison of Stylized Image and Original Content</a>
  <ul class="collapse">
  <li><a href="#code-and-its-description-4" id="toc-code-and-its-description-4" class="nav-link" data-scroll-target="#code-and-its-description-4">Code and its Description:</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Neural Image Style Transfer using PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><img src="https://archive.org/download/deep-learning-with-py-torch/Deep%20Learning%20with%20PyTorch.png" class="img-fluid"></p>
<section id="introduction-and-overview" class="level2">
<h2 class="anchored" data-anchor-id="introduction-and-overview">Introduction and Overview</h2>
<p>Neural Image Style Transfer, a fascinating application of deep learning, involves the fusion of artistic style from one image onto the content of another. Leveraging the power of PyTorch, this project delves into the realm of artistic transformation, exploring the amalgamation of visual styles through convolutional neural networks.</p>
<p>The core concept involves utilizing a pre-trained VGG19 model from PyTorch’s torchvision library. This model serves as the foundation for extracting features that represent the content and style of input images. Through a series of meticulously designed functions, the project performs image preprocessing, defining loss functions for both content and style, and orchestrating a training loop to optimize a target image that exhibits the desired content with the stylized features of another image.</p>
<p>By employing a blend of content and style loss functions and optimizing via an Adam optimizer, this project iteratively refines the target image, resulting in captivating visual amalgamations that showcase the content of one image infused with the artistic essence of another.</p>
<p>This Jupyter Notebook project encapsulates the essence of neural style transfer, offering a comprehensive and practical implementation using PyTorch, paving the way for creative exploration and experimentation in the realm of artificial intelligence and image transformation.</p>
</section>
<section id="loading-and-preparing-pretrained-vgg19-model-for-neural-style-transfer" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-preparing-pretrained-vgg19-model-for-neural-style-transfer">Loading and Preparing Pretrained VGG19 Model for Neural Style Transfer</h2>
<p>The “Loading Pretrained VGG Model” section focuses on importing and configuring the VGG19 model, which serves as the backbone for extracting features in the neural style transfer process.</p>
<section id="code-and-its-description" class="level3">
<h3 class="anchored" data-anchor-id="code-and-its-description">Code and its Description:</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load VGG19 pretrained model</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>vgg <span class="op">=</span> models.vgg19(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vgg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)</code></pre>
</div>
</div>
<p>This segment initializes the VGG19 model pretrained on ImageNet and prints its architecture summary, showcasing the layers and structure of the VGG19 model.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract just the features part of the VGG model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>vgg_features <span class="op">=</span> vgg.features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, only the feature extraction part of the VGG model is retained by assigning <code>vgg.features</code> to <code>vgg_features</code>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze parameters in the features network</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> vgg_features.parameters():</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    param.requires_grad_(<span class="va">False</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for GPU availability and move the model to the device</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>vgg_features.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cpu</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace=True)
  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (24): ReLU(inplace=True)
  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (26): ReLU(inplace=True)
  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (31): ReLU(inplace=True)
  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (33): ReLU(inplace=True)
  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (35): ReLU(inplace=True)
  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)</code></pre>
</div>
</div>
<p>This code snippet freezes the parameters (weights) of the feature extraction part to prevent them from being updated during training. It also checks for GPU availability and moves the VGG feature extraction module to the available device (GPU if available, else CPU) for computation.</p>
</section>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose:</h3>
<p>The purpose of this section is to set up the VGG19 model, which acts as a feature extractor in the neural style transfer process. By loading a pretrained model and extracting only the feature extraction part while freezing its weights, it ensures that during style transfer optimization, only the target image’s pixels are adjusted to match the style and content, utilizing the VGG model solely for feature extraction without altering its learned representations. Moreover, the model is moved to the appropriate device for efficient computation.</p>
</section>
</section>
<section id="image-preprocessing-transforming-raw-images-into-compatible-tensors" class="level2">
<h2 class="anchored" data-anchor-id="image-preprocessing-transforming-raw-images-into-compatible-tensors">Image Preprocessing: Transforming Raw Images into Compatible Tensors</h2>
<p>The “Image Preprocessing” section is dedicated to preparing raw images for neural style transfer by converting them into normalized tensors. This step involves several transformations to ensure compatibility with deep learning models while maintaining essential image features.</p>
<section id="code-and-its-description-1" class="level3">
<h3 class="anchored" data-anchor-id="code-and-its-description-1">Code and its Description:</h3>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms <span class="im">as</span> T</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_image(img_path, max_size<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(img_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> max_size <span class="cf">if</span> <span class="bu">max</span>(img.size) <span class="op">&gt;</span> max_size <span class="cf">else</span> <span class="bu">max</span>(img.size)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    img_transforms <span class="op">=</span> T.Compose([</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        T.Resize(size),</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        T.ToTensor(),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        T.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> img_transforms(img)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code defines the <code>preprocess_image</code> function responsible for image preprocessing. It loads the image from the specified path, converts it to the RGB format, resizes it while maintaining its aspect ratio based on the <code>max_size</code> parameter, converts it into a PyTorch tensor, and applies normalization using specific mean and standard deviation values.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess content and style images</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>content_image <span class="op">=</span> preprocess_image(<span class="st">"./Project-NST/content11.jpg"</span>).to(device)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>style_image <span class="op">=</span> preprocess_image(<span class="st">"./Project-NST/style11.jpg"</span>).to(device)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Content shape:"</span>, content_image.shape)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Style shape:"</span>, style_image.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Content shape: torch.Size([1, 3, 259, 345])
Style shape: torch.Size([1, 3, 500, 577])</code></pre>
</div>
</div>
<p>Here, the code applies the <code>preprocess_image</code> function to the content and style images (<code>content11.jpg</code> and <code>style11.jpg</code>). It then prints the shapes of the resulting preprocessed tensors to verify the successful transformation of the images into compatible tensors for neural style transfer.</p>
</section>
</section>
<section id="image-deprocessing-transforming-normalized-tensors-to-displayable-images" class="level2">
<h2 class="anchored" data-anchor-id="image-deprocessing-transforming-normalized-tensors-to-displayable-images">Image Deprocessing: Transforming Normalized Tensors to Displayable Images</h2>
<p>The “Image Deprocessing” section serves to reverse the normalization process applied to the images, converting normalized tensors back into displayable images suitable for visualization. This step is crucial after preprocessing images for neural style transfer to interpret and visualize the processed content and style images.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deprocess_image(tensor):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tensor.to(<span class="st">"cpu"</span>).clone().detach().numpy().squeeze()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.transpose(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image <span class="op">*</span> np.array([<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]) <span class="op">+</span> np.array([<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> np.clip(image, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>content_deprocessed <span class="op">=</span> deprocess_image(content_image)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>style_deprocessed <span class="op">=</span> deprocess_image(style_image)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Content shape:"</span>, content_deprocessed.shape)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Style shape:"</span>, style_deprocessed.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Content shape: (259, 345, 3)
Style shape: (500, 577, 3)</code></pre>
</div>
</div>
<p>This code defines the <code>deprocess_image</code> function responsible for reversing the normalization applied during preprocessing. It takes a tensor as input and performs operations to restore the image to its original form suitable for display. Then, the function is applied to the preprocessed content and style images (<code>content_image</code> and <code>style_image</code>) to obtain deprocessed images.</p>
<section id="visualization" class="level4">
<h4 class="anchored" data-anchor-id="visualization">Visualization:</h4>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ax1.imshow(content_deprocessed)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ax2.imshow(style_deprocessed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f88507fd990&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="nb_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This code snippet visualizes the deprocessed content and style images using Matplotlib to display the images side by side in a single figure, allowing a visual comparison between the original content and style images after deprocessing.</p>
</section>
</section>
<section id="feature-extraction-and-gram-matrix-calculation-analyzing-image-features-for-style-representation" class="level2">
<h2 class="anchored" data-anchor-id="feature-extraction-and-gram-matrix-calculation-analyzing-image-features-for-style-representation">Feature Extraction and Gram Matrix Calculation: Analyzing Image Features for Style Representation</h2>
<p>The “Feature Extraction and Gram Matrix Calculation” section focuses on extracting relevant features from the content and style images using a pre-trained model, followed by the computation of Gram matrices for style representation.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_features(image, model):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> {</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"0"</span>: <span class="st">"conv1_1"</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"5"</span>: <span class="st">"conv2_1"</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"10"</span>: <span class="st">"conv3_1"</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"19"</span>: <span class="st">"conv4_1"</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"21"</span>: <span class="st">"conv4_2"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"28"</span>: <span class="st">"conv5_1"</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> {}</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> image</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, layer <span class="kw">in</span> model._modules.items():</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> name <span class="kw">in</span> layers:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            features[layers[name]] <span class="op">=</span> x</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gram_matrix(tensor):</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    b, c, h, w <span class="op">=</span> tensor.size()</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> tensor.view(c, h<span class="op">*</span>w)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    gram <span class="op">=</span> torch.mm(tensor, tensor.t())</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gram</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Get content and style features</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>content_features <span class="op">=</span> get_features(content_image, vgg_features)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>style_features <span class="op">=</span> get_features(style_image, vgg_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="./images/nwxnh.png" class="img-fluid"></p>
<p><img src="./images/gram.webp" class="img-fluid"></p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create style Gram matrices</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>style_grams <span class="op">=</span> {layer: gram_matrix(style_features[layer]) <span class="cf">for</span> layer <span class="kw">in</span> style_features}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Feature Extraction (<code>get_features</code>):</strong> This function <code>get_features</code> takes an image tensor (<code>content_image</code> or <code>style_image</code>) and a pre-trained model (<code>vgg_features</code>). It iterates through the layers of the model, capturing the activations at specific layers defined in the <code>layers</code> dictionary. The extracted features are stored in the <code>content_features</code> and <code>style_features</code> dictionaries for the content and style images, respectively.</p>
<p><strong>Gram Matrix Calculation (<code>gram_matrix</code>):</strong> The <code>gram_matrix</code> function computes the Gram matrix for a given tensor. It reshapes the tensor and performs matrix multiplication to calculate the Gram matrix, which represents the correlations between different feature maps in a layer.</p>
<p><strong>Style Gram Matrices:</strong> The code snippet <code>style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}</code> computes the Gram matrices for the style features extracted from various layers of the style image.</p>
<p>This section is pivotal in extracting meaningful features from the content and style images and further processing them into Gram matrices, which play a crucial role in defining the style representation required for neural style transfer. These matrices capture the correlations among different features, contributing to the artistic style representation of the style image.</p>
</section>
<section id="defining-loss-functions-for-neural-style-transfer" class="level2">
<h2 class="anchored" data-anchor-id="defining-loss-functions-for-neural-style-transfer">Defining Loss Functions for Neural Style Transfer</h2>
<p>In the “Creating Style and Content Loss Function” section, the code establishes two key loss functions—content loss and style loss—integral to the optimization process in neural style transfer.</p>
<section id="content-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="content-loss-function">Content Loss Function:</h3>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> content_loss(target_conv4_2, content_conv4_2):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.mean((target_conv4_2 <span class="op">-</span> content_conv4_2) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>content_loss</code> function computes the content loss between the target image’s specific convolutional layer activations (<code>target_conv4_2</code>) and the content image’s activations (<code>content_conv4_2</code>). It calculates the mean squared difference between these activations, representing the deviation in content information.</p>
</section>
<section id="style-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="style-loss-function">Style Loss Function:</h3>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> style_loss(weights, target_features, style_grams):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> weights:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        target_feature <span class="op">=</span> target_features[layer]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        target_gram <span class="op">=</span> gram_matrix(target_feature)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        style_gram <span class="op">=</span> style_grams[layer]</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        b, c, h, w <span class="op">=</span> target_feature.shape</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        layer_loss <span class="op">=</span> weights[layer] <span class="op">*</span> torch.mean((target_gram <span class="op">-</span> style_gram) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> layer_loss <span class="op">/</span> (c <span class="op">*</span> h <span class="op">*</span> w)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>style_weights <span class="op">=</span> {</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"conv1_1"</span>: <span class="fl">1.0</span>,</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"conv2_1"</span>: <span class="fl">0.75</span>,</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"conv3_1"</span>: <span class="fl">0.2</span>,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"conv4_1"</span>: <span class="fl">0.2</span>,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"conv5_1"</span>: <span class="fl">0.2</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>style_loss</code> function calculates the style loss by iterating through specified layers, computing the Gram matrices for the target and style images, and measuring the mean squared difference between their Gram matrices. This captures the stylistic differences between the target and style images across multiple layers with weighted contributions specified in <code>style_weights</code>.</p>
</section>
<section id="optimization-and-total-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="optimization-and-total-loss-function">Optimization and Total Loss Function:</h3>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage of content and style loss</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>target_image <span class="op">=</span> content_image.clone().requires_grad_(<span class="va">True</span>).to(device)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>target_features <span class="op">=</span> get_features(target_image, vgg_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam([target_image], lr<span class="op">=</span><span class="fl">0.003</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">1e5</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> total_loss(alpha, beta, style_loss, content_loss):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha <span class="op">*</span> content_loss <span class="op">+</span> beta <span class="op">*</span> style_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The code sets up an Adam optimizer for the target image and defines <code>alpha</code> and <code>beta</code> as weights to balance the content and style losses. The <code>total_loss</code> function combines the content and style losses with their respective weights to form the overall loss function that the optimization process aims to minimize.</p>
<p>This section establishes the mathematical frameworks for quantifying content and style differences between images, essential for guiding the neural style transfer optimization process. Adjusting <code>alpha</code> and <code>beta</code> enables fine-tuning the emphasis between content and style in the generated images.</p>
</section>
</section>
<section id="training-loop-iterative-optimization-for-neural-style-transfer" class="level2">
<h2 class="anchored" data-anchor-id="training-loop-iterative-optimization-for-neural-style-transfer">Training Loop: Iterative Optimization for Neural Style Transfer</h2>
<p>The “Training Loop” section orchestrates the iterative optimization process used in neural style transfer to generate a stylized image. This loop iterates through multiple epochs, adjusting the target image to minimize the total loss, a combination of content and style losses, using optimization techniques.</p>
<section id="code-and-its-description-2" class="level3">
<h3 class="anchored" data-anchor-id="code-and-its-description-2">Code and its Description:</h3>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>show_every <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    target_features <span class="op">=</span> get_features(target_image, vgg_features)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    content_loss_val <span class="op">=</span> content_loss(target_features[<span class="st">"conv4_2"</span>], content_features[<span class="st">"conv4_2"</span>])</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    style_loss_val <span class="op">=</span> style_loss(style_weights, target_features, style_grams)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    total_loss_val <span class="op">=</span> total_loss(alpha, beta, style_loss_val, content_loss_val)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    total_loss_val.backward()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> show_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Total loss at Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>total_loss_val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        results.append(deprocess_image(target_image.detach()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total loss at Epoch 0: 280201792.0
Total loss at Epoch 500: 20301662.0
Total loss at Epoch 1000: 9346065.0
Total loss at Epoch 1500: 7137745.0
Total loss at Epoch 2000: 5871047.5
Total loss at Epoch 2500: 5056373.0</code></pre>
</div>
</div>
<p>This code initiates a loop that iterates over a specified number of epochs (<code>epochs</code>) to optimize the target image. Within each iteration: - The features of the current target image are extracted using <code>get_features</code>. - Content loss (<code>content_loss_val</code>) and style loss (<code>style_loss_val</code>) are computed based on the target image’s features and precomputed content and style features. - The total loss (<code>total_loss_val</code>), combining content and style losses, is calculated. - The optimizer’s gradient is reset, and the total loss is used to perform backpropagation (<code>total_loss_val.backward()</code>) and update the target image (<code>optimizer.step()</code>).</p>
<p>Additionally, at specified intervals (<code>show_every</code>), the current total loss is printed, providing insights into the optimization process. The stylized images at these intervals are also captured and stored in <code>results</code> for visualization.</p>
</section>
</section>
<section id="results-visualization-displaying-stages-of-style-transfer-process" class="level2">
<h2 class="anchored" data-anchor-id="results-visualization-displaying-stages-of-style-transfer-process">Results Visualization: Displaying Stages of Style Transfer Process</h2>
<p>The “Results Visualization” section aims to showcase the progression of the style transfer algorithm by displaying multiple stages of the generated images throughout the training process. This allows for a visual inspection of how the stylized image evolves over different epochs or iterations during the neural style transfer.</p>
<section id="code-and-its-description-3" class="level3">
<h3 class="anchored" data-anchor-id="code-and-its-description-3">Code and its Description:</h3>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(results)):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(results[i])</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nb_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The code snippet sets up a Matplotlib figure to display the generated images resulting from the style transfer process. It iterates through the <code>results</code> list containing images obtained at different stages of training. For each image in the <code>results</code> list, it plots it in a subplot within the Matplotlib figure, arranging the images in a grid format.</p>
</section>
</section>
<section id="final-results-comparison-of-stylized-image-and-original-content" class="level2">
<h2 class="anchored" data-anchor-id="final-results-comparison-of-stylized-image-and-original-content">Final Results: Comparison of Stylized Image and Original Content</h2>
<p>The “Final Results” section encapsulates the comparison between the stylized image generated through neural style transfer and the original content image. It enables a visual assessment of the transformation achieved by the neural network, showcasing the artistic style transferred onto the content image.</p>
<section id="code-and-its-description-4" class="level3">
<h3 class="anchored" data-anchor-id="code-and-its-description-4">Code and its Description:</h3>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>target_final <span class="op">=</span> deprocess_image(target_image.detach())</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>content_copy <span class="op">=</span> deprocess_image(content_image)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>ax1.imshow(target_final)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>ax2.imshow(content_copy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f8847b52ed0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="nb_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This code snippet depicts the final stage of the neural style transfer process. It applies the <code>deprocess_image</code> function to the stylized image (<code>target_image</code>) and the original content image (<code>content_image</code>). Subsequently, it utilizes Matplotlib to exhibit these images side by side within a single figure for easy comparison.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The journey through this notebook on Neural Image Style Transfer using PyTorch has been an intriguing exploration into the fusion of art and artificial intelligence. Leveraging the capabilities of pre-trained VGG19 models, this project embarked on a creative endeavor, showcasing the amalgamation of visual styles from one image onto the content of another.</p>
<p>By harnessing the power of deep learning and convolutional neural networks, this endeavor unveiled the essence of content and style representations within images, paving the way for captivating transformations. Through meticulous preprocessing, feature extraction, loss function definitions, and iterative optimization, this notebook illustrated the step-by-step process of generating stylized images, evolving dynamically over epochs.</p>
<p>The visual journey depicted the progressive refinement of the stylized image, capturing the essence of artistic style transferred onto the original content. This journey, driven by the interplay of content and style losses, demonstrated the network’s ability to distill and blend artistic characteristics into a visually appealing fusion.</p>
<p>In essence, this notebook encapsulates the intricate orchestration of machine learning techniques, showcasing the neural style transfer process, and offers a gateway for further exploration and experimentation in the realms of artificial intelligence, creativity, and image transformation.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>